[00:00:00.020] -> Wir kommen,

[00:00:00.540] -> liebe Zuhörerinnen und Zuhörer,

[00:00:02.720] -> zu diesem Hap-Impuls zum Thema Medien und Demokratie.

[00:00:06.400] -> Wir stellen uns heute eine Frage,

[00:00:07.799] -> die uns alle betrifft und die wirklich

[00:00:10.119] -> im Kern unserer Gesellschaft rüttelt.

[00:00:12.300] -> Was passiert eigentlich mit unserer Demokratie,

[00:00:14.640] -> wenn wir nicht mehr alle die gleichen Nachrichten sehen?

[00:00:17.399] -> Wir gehen da zusammen auf eine kleine Reise,

[00:00:19.480] -> weg vom sprichwörtlichen Dorfplatz von früher,

[00:00:22.260] -> der Agora,

[00:00:23.120] -> wo alle zusammen kamen.

[00:00:24.420] -> Und mitten rein in diese Welt der personalisierten Fiez,

[00:00:28.260] -> wo jeder seine eigene Realität auf dem Handy hat.

[00:00:30.879] -> Genau.

[00:00:31.300] -> Und um diese komplexe Entwicklung zu verstehen,

[00:00:34.600] -> nehmen wir dich mal mit auf einen Weg.

[00:00:36.200] -> Wir gehen das in drei Schritten an.

[00:00:38.119] -> Okay.

[00:00:38.740] -> Zuerst schauen wir uns die Theorie an.

[00:00:40.840] -> Also die grundlegende Mechanik dahinter.

[00:00:43.219] -> Dann im zweiten Schritt prüfen wir die harte Datenlage.

[00:00:46.460] -> Also die Zahlen.

[00:00:47.320] -> Genau.

[00:00:47.659] -> Was sagen die Zahlen wirklich?

[00:00:49.759] -> Und zum Schluss wagen wir uns an die große Frage.

[00:00:53.119] -> Was sind die Folgen für uns alle?

[00:00:55.320] -> Okay, lass uns das mal auspacken.

[00:00:57.240] -> Der Kern des Ganzen ist ja eine massive Machtverschiebung.

[00:01:00.320] -> Früher,

[00:01:00.640] -> da saßen in den Zeitungsredaktionen Menschen,

[00:01:02.939] -> die haben entschieden.

[00:01:03.899] -> Das hier ist wichtig.

[00:01:04.780] -> Und das ist nur ein Gerücht.

[00:01:06.060] -> Genau.

[00:01:06.680] -> Und heute?

[00:01:07.599] -> Heute entscheidet immer öfter ein

[00:01:09.069] -> unsichtbarer Algorithmus auf deinem Handy,

[00:01:11.359] -> was du überhaupt zu sehen bekommst.

[00:01:12.920] -> Und dieser kleine Unterschied verändert,

[00:01:15.140] -> wie wir die Welt wahrnehmen.

[00:01:16.140] -> Komplett.

[00:01:16.859] -> Stell dir mal vor,

[00:01:17.560] -> früher saßen fast alle Abend zum Acht vor dem Fernseher.

[00:01:20.439] -> Tagesschau?

[00:01:21.379] -> Die Tagesschau.

[00:01:22.099] -> Genau.

[00:01:22.680] -> Das war ein gemeinsames Lagerfeuer.

[00:01:24.359] -> Man hat dieselben Bilder gesehen, dieselben

[00:01:26.299] -> Infos bekommen.

[00:01:27.260] -> Man konnte sich danach darüber streiten.

[00:01:29.079] -> Klar.

[00:01:29.420] -> Aber man hat sich über dieselbe

[00:01:31.599] -> Sache gestritten.

[00:01:32.700] -> Es gab eine gemeinsame Basis.

[00:01:34.579] -> Und der Kontrast dazu ist heute der eigene

[00:01:37.019] -> Tunnel.

[00:01:37.579] -> Ja. Dein TikTok-Feed ist ein anderer als meiner.

[00:01:41.079] -> Dein Instagram ist anders als

[00:01:42.540] -> das deiner Kollegen.

[00:01:44.000] -> Jeder schaut quasi durch seinen eigenen personalisierten Tunnel.

[00:01:47.340] -> Der

[00:01:47.579] -> Tunnel wurde von Algorithmen gebaut.

[00:01:49.099] -> Und hier kommt der springende Punkt.

[00:01:51.060] -> Das Ziel dieses

[00:01:51.959] -> Algorithmus ist nicht,

[00:01:53.219] -> dich umfassend und neutral zu informieren.

[00:01:55.659] -> Sondern?

[00:01:56.379] -> Sein Ziel ist es, dich

[00:01:57.500] -> so lange wie möglich in diesem Tunnel zu halten.

[00:02:00.319] -> Deine Aufmerksamkeit zu fesseln.

[00:02:02.140] -> Ist dir das

[00:02:02.480] -> schon mal aufgefallen?

[00:02:03.859] -> Du sprichst mit Freunden über eine große News und merkst,

[00:02:06.780] -> dass sie

[00:02:07.019] -> in ihrem Feed was komplett anderes dazu gesehen haben.

[00:02:10.319] -> Oder vielleicht sogar gar nichts.

[00:02:11.740] -> Das

[00:02:12.099] -> passiert andauernd.

[00:02:13.000] -> Das führt uns zum nächsten Begriff.

[00:02:14.659] -> Die Agora.

[00:02:15.500] -> Das war im alten Klichenland

[00:02:17.159] -> der Marktplatz,

[00:02:18.419] -> das Zentrum des öffentlichen Lebens.

[00:02:20.379] -> Da wurde gehandelt, aber eben auch debattiert,

[00:02:23.319] -> gestritten.

[00:02:24.360] -> Da ist Politik überhaupt erst entstanden,

[00:02:26.800] -> Öffentlichkeit.

[00:02:27.719] -> Genau.

[00:02:28.319] -> Und der

[00:02:28.539] -> Unterschied zu heute,

[00:02:30.000] -> der lässt sich mit einer Analogie gut beschreiben.

[00:02:32.460] -> Die alte Öffentlichkeit

[00:02:33.680] -> war wie eine große Schulversammlung in der Aula.

[00:02:36.479] -> Okay.

[00:02:37.159] -> Der Direktor hält eine Durchsage.

[00:02:39.159] -> Und ob es dich jetzt interessiert oder nicht?

[00:02:41.000] -> Alle hören dieselbe Information.

[00:02:42.520] -> Richtig.

[00:02:43.259] -> Und heute?

[00:02:44.300] -> Heute ist das eher wie tausende kleiner,

[00:02:46.599] -> privater WhatsApp-Gruppen,

[00:02:48.280] -> die gleichzeitig laufen.

[00:02:49.580] -> Und in jeder Gruppe wird über was anderes geredet.

[00:02:51.699] -> Mit eigenen Wahrheiten.

[00:02:53.000] -> Genau.

[00:02:53.560] -> Die große,

[00:02:54.400] -> gemeinsame Versammlung,

[00:02:55.699] -> die findet kaum noch statt.

[00:02:57.020] -> Und in dieser alten Aula,

[00:02:58.659] -> da gab es die sogenannten Gatekeeper,

[00:03:00.699] -> die Torwächter.

[00:03:01.620] -> Und haben entschieden,

[00:03:02.740] -> diese Information ist geprüft und relevant,

[00:03:05.500] -> die darf rein.

[00:03:06.659] -> Eine andere ist unbelegt,

[00:03:08.300] -> ein Gerücht,

[00:03:09.240] -> die bleibt draußen.

[00:03:10.539] -> Heute steht dieses Tor oft sperrangelweit oft.

[00:03:14.219] -> Also jeder kann alles posten.

[00:03:15.960] -> Im Prinzip, ja.

[00:03:17.020] -> Egal,

[00:03:17.300] -> ob es eine recherchierte Tatsache ist,

[00:03:19.979] -> eine Meinung oder eine gezielte

[00:03:21.900] -> Falschinformation.

[00:03:23.000] -> Diese menschliche Qualitätskontrolle,

[00:03:24.840] -> die fällt in großen Teilen einfach weg.

[00:03:27.300] -> Und hier wird es jetzt wirklich interessant,

[00:03:29.979] -> wenn wir uns den Algorithmus genauer ansehen.

[00:03:32.219] -> Er wird oft als Ja-Saga beschrieben.

[00:03:34.759] -> Das trifft es perfekt.

[00:03:35.780] -> Er will dich ja nicht daraus fordern.

[00:03:37.439] -> Er will dir nicht andere Meinungen zeigen.

[00:03:39.180] -> Nein.

[00:03:39.939] -> Er will dir mehr von dem geben,

[00:03:41.159] -> was du eh schon magst.

[00:03:42.419] -> Dich bestätigen.

[00:03:43.240] -> Warum?

[00:03:43.860] -> Weil er gelernt hat.

[00:03:45.080] -> Inhalte, die starke Emotionen auslösen.

[00:03:47.560] -> Vor allem Wut und Empörung.

[00:03:49.060] -> Aber auch große Freude.

[00:03:50.280] -> Die binden deine Aufmerksamkeit viel effektiver als eine,

[00:03:53.939] -> sagen wir, trockene sachliche Analyse.

[00:03:56.180] -> Dahinter steckt das Geschäftsmodell der Aufmerksamkeitsökonomie.

[00:04:00.060] -> Deine Aufmerksamkeit ist die Währung.

[00:04:02.099] -> Jede Sekunde,

[00:04:02.719] -> die du länger auf einer Plattform bleibst,

[00:04:04.860] -> ist purees Geld wert.

[00:04:06.580] -> Okay,

[00:04:07.300] -> das ist die Mechanik dahinter.

[00:04:08.539] -> Klingt alles logisch,

[00:04:09.699] -> aber auch ein bisschen abstrakt.

[00:04:11.759] -> Die Frage ist ja,

[00:04:12.719] -> lässt sich das in harten Zahlen nachweisen?

[00:04:15.539] -> Absolut.

[00:04:16.360] -> Und das bringt uns zum zweiten großen Block.

[00:04:19.139] -> Der Datenlage.

[00:04:20.379] -> Was sagen denn die Zahlen?

[00:04:21.420] -> Die sprechen eine sehr, sehr deutliche Sprache.

[00:04:24.540] -> Eine Erhebung aus Deutschland zeigt zum Beispiel,

[00:04:27.000] -> noch 43 Prozent der Gesamtbevölkerung nutzen

[00:04:29.879] -> klassisches Fernsehen für Nachrichten.

[00:04:32.379] -> Das alte Lagerfeuer.

[00:04:33.540] -> Genau.

[00:04:34.279] -> Bei den 18 bis 24-Jährigen sind es aber bereits 50 Prozent,

[00:04:38.480] -> die sich hauptsächlich

[00:04:39.420] -> über Social Media informieren.

[00:04:40.839] -> Die Hälfte.

[00:04:41.459] -> Die Hälfte.

[00:04:42.220] -> Das ist nicht nur ein kleiner Trend,

[00:04:44.040] -> das ist ein fundamentaler Generationenwechsel.

[00:04:46.939] -> Die Jungen leben bereits in einer komplett anderen Informationswelt.

[00:04:50.740] -> Und das führt ja direkt zu diesem 15 Sekunden-Problem.

[00:04:54.000] -> Die Frage ist provokant, aber...

[00:04:55.839] -> Aber berechtigt.

[00:04:57.579] -> Kannst du eine komplexe Vorlage wie eine Rentenreform

[00:04:59.959] -> wirklich in einem 15-sekündigen TikTok-Video

[00:05:02.620] -> verstehen?

[00:05:03.259] -> Wahrscheinlich nicht.

[00:05:04.199] -> Was du bekommst,

[00:05:05.000] -> ist die Emotion,

[00:05:06.079] -> der Slogan,

[00:05:07.100] -> die Zuspitzung,

[00:05:08.139] -> aber die Details,

[00:05:09.300] -> die Grautöne, die Argumente der Gegenseite.

[00:05:11.899] -> Also alles, was du für eine

[00:05:13.180] -> fundierte demokratische Entscheidung brauchst.

[00:05:15.480] -> Das bleibt auf der Strecke.

[00:05:16.860] -> Man nennt das auch

[00:05:17.500] -> die Tiktokisierung der Politik.

[00:05:19.860] -> Komplexe Sachverhalte werden auf extrem kurze und behaltsame Häppchen

[00:05:23.860] -> untergebrochen.

[00:05:24.839] -> Der Kontext geht verloren.

[00:05:26.180] -> Total.

[00:05:26.920] -> Und die Daten zeigen noch etwas Beunruhigendes.

[00:05:29.800] -> Eine wachsende Bildungskluft.

[00:05:31.819] -> Menschen mit niedrigerer formaler Bildung neigen dazu,

[00:05:35.199] -> Nachrichten komplett zu meiden.

[00:05:37.000] -> Warum das?

[00:05:37.779] -> Sie fühlen sich überfordert,

[00:05:39.139] -> nicht angesprochen oder misstrauen den Medien.

[00:05:42.060] -> Und das vergrößert die Kluft zwischen

[00:05:43.740] -> informierten und nicht informierten noch weiter.

[00:05:46.899] -> Jetzt kommt ein Punkt,

[00:05:47.740] -> der oft übersehen wird,

[00:05:49.079] -> aber absolut zentral ist.

[00:05:50.860] -> Das Geld.

[00:05:51.839] -> Jedes Jahr fließend Milliarden Franken allein aus der Schweiz.

[00:05:55.079] -> Direkt an globale Tech-Dat-Form.

[00:05:57.060] -> Google, Meta, Tiktok.

[00:05:58.980] -> Und dieses Geld ist ja nicht einfach nur weg,

[00:06:00.980] -> es fehlt an einem anderen Ort.

[00:06:02.680] -> Bei den Schweizer Medienhäusern.

[00:06:04.300] -> Genau.

[00:06:04.899] -> Und das erzeugt einen echten Teufelskreis.

[00:06:07.519] -> Eine Abwärtsspirale.

[00:06:08.879] -> Stell dir das so vor.

[00:06:09.920] -> Dieses Geld fehlt,

[00:06:10.899] -> um teuren,

[00:06:11.620] -> aber wichtigen Qualitätsjournalismus zu finanzieren.

[00:06:14.519] -> Also die Recherchen,

[00:06:15.660] -> die Missstände aufdecken oder komplexe Vorlagen verständlich machen.

[00:06:19.319] -> Richtig.

[00:06:20.120] -> Weil das Geld fehlt,

[00:06:21.339] -> sinkt die Qualität oder die Vielfalt.

[00:06:23.360] -> Daraufhin wenden sich die Leute noch stärker den

[00:06:25.620] -> schnellen kostenlosen Social-Media-Häppchen zu.

[00:06:28.480] -> Was wiederum noch mehr Werbegelder zu den Tech-Giganten lenkt.

[00:06:31.740] -> Ein klassischer Teufelskreis.

[00:06:33.699] -> Und der hüllt den lokalen Journalismus langsam aber sicher aus.

[00:06:37.180] -> Das führt uns jetzt zum dritten Block.

[00:06:38.980] -> Was sind die Folgen?

[00:06:40.259] -> Was macht das mit unserer Demokratie?

[00:06:42.220] -> Ganz besonders mit einer direkten Demokratie,

[00:06:44.660] -> wie der unseren in der Schweiz,

[00:06:46.079] -> wo wir ja alle paar Monate über extrem komplexe Gesetze abstimmen.

[00:06:49.779] -> Drei sehr konkrete Folgen.

[00:06:52.019] -> Die erste ist der sogenannte Slack-Tivisen.

[00:06:55.160] -> Ein Kunstwort.

[00:06:56.339] -> Ja, aus Slacker, also Faulenzer und Aktivismus.

[00:07:00.360] -> Das beschreibt das Phänomen,

[00:07:01.879] -> dass du einen politischen Post likes

[00:07:03.660] -> oder teilst und dich dabei gut fühlst.

[00:07:06.139] -> Als hättest du dich engagiert.

[00:07:07.899] -> Genau.

[00:07:08.519] -> Aber dieses Gefühl von Aktivität ersetzt

[00:07:11.199] -> immer öfter die echte,

[00:07:12.579] -> die mühevolle politische Teilhabe.

[00:07:14.480] -> Also zur Gemeindeversamlung gehen oder sich die Zeit nehmen,

[00:07:17.259] -> zu ohne zu gehen.

[00:07:18.180] -> Exakt.

[00:07:18.699] -> Die zweite Folge ist die Anfälligkeit für Fakes.

[00:07:22.040] -> Und die Zahl dazu ist wirklich alarmieren.

[00:07:23.980] -> Ja, die ist heftig.

[00:07:25.600] -> Nur 55 Prozent der Schweizerinnen und Schweizer geben an,

[00:07:29.540] -> dass sie falsch Nachrichten

[00:07:30.759] -> zuverlässig erkennen können.

[00:07:32.180] -> Das heißt ja im Umkehrschluss,

[00:07:33.360] -> dass fast die Hälfte von uns

[00:07:34.560] -> bei dem Thema auf Glatteis ist.

[00:07:36.319] -> Bei einer knappen Abstimmung ist das wie ein Münzwurf,

[00:07:40.079] -> wessen Desinformation sich am Ende durchsetzt.

[00:07:42.180] -> Das ist ein enormes Risiko.

[00:07:44.399] -> Und die dritte Folge ist die

[00:07:46.860] -> affektive Polarisierung.

[00:07:48.420] -> Das klingt technisch,

[00:07:49.779] -> meint aber was sehr Menschliches und auch Gefährliches.

[00:07:53.079] -> Was genau heißt das?

[00:07:54.339] -> Es bedeutet,

[00:07:55.420] -> dass wir den politischen Gegner nicht mehr nur als jemanden mit

[00:07:58.139] -> einer anderen Meinung sehen,

[00:07:59.399] -> sondern wir sehen ihn zunehmend als schlechten,

[00:08:01.959] -> dummen oder sogar bösen

[00:08:03.639] -> Menschen.

[00:08:04.079] -> Es geht nicht mehr um die Sache,

[00:08:05.560] -> es geht um die Person.

[00:08:06.920] -> Genau.

[00:08:07.319] -> Und die Algorithmen befeuern

[00:08:08.899] -> das, weil, wir haben es gehört,

[00:08:11.040] -> Wut und Empörung erzeugen mehr Klicks als eine sachliche Debatte.

[00:08:14.920] -> Die

[00:08:15.100] -> Gräben werden tiefer.

[00:08:16.360] -> Und das alles mündet in einer massiven Machtverschiebung.

[00:08:19.360] -> Wer in

[00:08:19.660] -> unserer Gesellschaft überhaupt bestimmt worüber wir reden?

[00:08:22.439] -> Das nennt man Agenda Setting.

[00:08:24.040] -> Ja,

[00:08:24.420] -> früher haben die großen Medien die Agenda gesetzt.

[00:08:27.000] -> Wenn etwas auf der Titelseite stand,

[00:08:29.079] -> wurde darüber gesprochen.

[00:08:30.279] -> Und heute?

[00:08:31.139] -> Heute läuft das oft genau umgekehrt.

[00:08:33.559] -> Das Beispiel

[00:08:34.080] -> der Covid-Abstimmungen hat das eindrücklich gezeigt.

[00:08:36.980] -> Themen entstehen und wachsen in geschlossenen

[00:08:39.720] -> Gruppen auf Telegram oder WhatsApp.

[00:08:41.679] -> Dort werden sie emotional aufgeladen, mit Müten

[00:08:44.720] -> angereichert.

[00:08:45.600] -> Bis sie eine so große Wucht entwickeln,

[00:08:47.759] -> dass sie in die breite Öffentlichkeit schwappen.

[00:08:50.220] -> Und dann müssen die traditionelle

[00:08:51.940] -> Politik und die Medien darauf reagieren.

[00:08:54.000] -> Sie setzen

[00:08:54.539] -> die Agenda also nicht mehr,

[00:08:56.039] -> sie rennen ihr hinterher.

[00:08:57.379] -> Genau.

[00:08:58.059] -> Die Politik diskutiert nicht

[00:08:59.259] -> mehr das,

[00:08:59.879] -> was wichtig ist,

[00:09:01.139] -> sondern das,

[00:09:01.879] -> was viral geht.

[00:09:03.299] -> Das bringt uns zum letzten und

[00:09:05.059] -> vielleicht wichtigsten Punkt.

[00:09:06.659] -> Wenn wir akzeptieren,

[00:09:08.259] -> dass der Algorithmus dich nicht

[00:09:10.179] -> objektiv informieren will,

[00:09:12.059] -> weil sein Ziel ein anderes ist,

[00:09:13.879] -> nämlich deine

[00:09:14.629] -> Aufmerksamkeit zu binden,

[00:09:16.220] -> wer trägt dann die Verantwortung dafür,

[00:09:18.600] -> dass du als

[00:09:19.159] -> Bürgerin,

[00:09:19.679] -> als Bürger trotzdem gut informiert bist.

[00:09:22.659] -> Wer ist verantwortlich,

[00:09:23.759] -> damit wir in unserer Demokratie mit entscheiden können?

[00:09:26.320] -> Es gibt im Grunde drei

[00:09:27.500] -> mögliche Antworten.

[00:09:28.899] -> Erstens sind es die Tech-Giganten selbst.

[00:09:31.659] -> Müssten sie gesetzlich verpflichtet werden,

[00:09:33.440] -> ihre Algorithmen anders zu gestalten?

[00:09:35.299] -> Zweitens ist es der Staat,

[00:09:36.840] -> der eingreifen und vielleicht Qualitätsmedien fördern muss?

[00:09:40.200] -> Oder drittens bist am Ende du selbst,

[00:09:43.820] -> der in der Verantwortung steht,

[00:09:45.379] -> sich aktiv zu informieren,

[00:09:47.240] -> Quellen zu prüfen und aus der eigenen Blase auszubrechen.