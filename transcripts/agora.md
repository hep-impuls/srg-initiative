[00:00:00.060] -> Hallo und herzlich willkommen.

[00:00:02.000] -> Lass uns heute mal gemeinsam drauf schauen,

[00:00:04.419] -> was passiert,

[00:00:05.559] -> wenn du und die Menschen um dich herum

[00:00:07.929] -> nicht mehr die gleichen Nachrichten sehen.

[00:00:10.259] -> Genau.

[00:00:10.939] -> Früher war das ja einfach,

[00:00:12.119] -> da gab es die Tagesschau,

[00:00:13.580] -> sozusagen das gemeinsame Lagerfeuer.

[00:00:16.160] -> Heute hat jeder seinen eigenen Feed,

[00:00:18.539] -> seinen eigenen Tunnel.

[00:00:19.960] -> Und da stellt sich für dich in der

[00:00:21.100] -> Schweiz halt die ganz zentrale Frage,

[00:00:24.059] -> was bedeutet das für eine direkte Demokratie,

[00:00:26.719] -> in der du ja ständig über echt komplexe Themen abstimmen musst?

[00:00:30.420] -> Absolut.

[00:00:31.239] -> Wir gehen das jetzt mal zusammen mit dir durch.

[00:00:33.320] -> Zuerst die Theorie dahinter,

[00:00:35.119] -> dann schauen wir uns die harten Zahlen an

[00:00:37.399] -> und am Ende die direkten Folgen.

[00:00:40.240] -> Super,

[00:00:40.679] -> lass uns direkt bei der Theorie einsteigen.

[00:00:42.979] -> Stell dir mal das alte Griechenland vor.

[00:00:45.399] -> Die Agora.

[00:00:46.359] -> Der große Marktplatz.

[00:00:47.880] -> Genau.

[00:00:48.880] -> Der eine zentrale Ort,

[00:00:50.240] -> wo alle zusammen kamen,

[00:00:51.439] -> um zu diskutieren.

[00:00:52.740] -> Wie eine riesige Schulversammlung.

[00:00:55.100] -> Und heute.

[00:00:56.539] -> Heute ist dieser eine Agurat zerfallen in,

[00:00:59.700] -> ich

[00:00:59.840] -> weiß nicht, tausende kleine WhatsApp-Gruppen.

[00:01:02.719] -> Ja,

[00:01:03.759] -> dieser Vergleich vom Lagerfeuer zum privaten

[00:01:06.239] -> Tunnel, der trifft es echt gut.

[00:01:08.400] -> Früher gab es bei den großen Medien ja den sogenannten

[00:01:11.099] -> Gatekeeper,

[00:01:12.159] -> also den Torwächter.

[00:01:13.200] -> Die Chefredakteurin zum Beispiel,

[00:01:14.519] -> richtig,

[00:01:14.900] -> die hat entschieden,

[00:01:15.780] -> das hier ist eine wichtige Nachricht

[00:01:17.519] -> und das da drüben ist nur ein Gerücht.

[00:01:20.180] -> Das wirft

[00:01:20.500] -> natürlich eine spannende Frage auf.

[00:01:22.959] -> Wer ist eigentlich heute der Gatekeeper in deinem

[00:01:26.159] -> Feed?

[00:01:27.219] -> Die ehrliche Antwort ist meistens ein Algorithmus.

[00:01:30.799] -> Und hier wird es knifflig.

[00:01:33.239] -> Ja. Sein

[00:01:34.400] -> Ziel ist ja nicht unbedingt,

[00:01:36.120] -> dich umfassend und neutral zu informieren.

[00:01:38.980] -> Sein Ziel ist

[00:01:39.640] -> deine Aufmerksamkeit.

[00:01:41.519] -> Er will einfach,

[00:01:42.280] -> dass du so lange wie möglich in der App bleibst.

[00:01:44.780] -> Exakt.

[00:01:45.680] -> Man könnte auch sagen,

[00:01:47.019] -> der Algorithmus ist dein persönlicher Hype-Mail.

[00:01:49.959] -> Er will

[00:01:50.200] -> dich nicht herausfordern,

[00:01:51.819] -> er will dich bestätigen.

[00:01:53.079] -> Ein Ja-Sager quasi.

[00:01:54.540] -> Genau.

[00:01:54.939] -> Ein Ja-Sager, der dich

[00:01:56.239] -> emotional binden will.

[00:01:58.019] -> Warum?

[00:01:58.879] -> Weil Wut oder Freude dich viel länger

[00:02:01.239] -> auf der Plattform halten als eine

[00:02:03.120] -> neutrale, vielleicht etwas trockene Tatsache.

[00:02:05.859] -> Es geht halt um Verweildauer, nicht um Wahrheit.

[00:02:09.800] -> Puh,

[00:02:10.180] -> das ist schon eine beunruhigende Vorstellung,

[00:02:11.860] -> wenn man bedenkt,

[00:02:12.659] -> dass da unsere politische

[00:02:13.979] -> Meinungsbildung stattfindet.

[00:02:15.199] -> Das ist die Theorie,

[00:02:17.080] -> aber das kann man auch in harten Zahlen sehen.

[00:02:18.800] -> Okay,

[00:02:19.319] -> dann lassen's mal auf die Datenlage schauen,

[00:02:21.199] -> da wird's nämlich sehr konkret.

[00:02:22.780] -> Nimm mal die erste Zahl.

[00:02:24.599] -> 46 Prozent.

[00:02:25.919] -> Fast die Hälfte der Schweizerinnen und Schweizer

[00:02:28.840] -> meidet aktiv harte Nachrichten.

[00:02:31.379] -> Wahnsinn, fast die Hälfte.

[00:02:32.699] -> Ja,

[00:02:33.460] -> man nennt diese Gruppe die News-Depriviierten.

[00:02:36.599] -> Die schalten einfach ab, weil ihnen die Flut

[00:02:38.879] -> zu negativ oder zu komplex ist.

[00:02:40.819] -> Und bei den Jüngeren ist der Trend ja noch deutlicher.

[00:02:43.699] -> In Deutschland zum Beispiel,

[00:02:45.300] -> da

[00:02:45.439] -> beziehen 17 Prozent der 18 bis 24-Jährigen

[00:02:49.080] -> ihre Infos wirklich nur noch über Social Media.

[00:02:52.000] -> Der klassische Journalismus kommt da gar nicht mehr an.

[00:02:54.560] -> Nein.

[00:02:55.560] -> Und die Plattform, die bei jungen

[00:02:57.539] -> Schweizern für News gerade komplett durch die Decke geht,

[00:03:00.599] -> ist halt TikTok.

[00:03:02.360] -> Und das führt

[00:03:02.979] -> zudem,

[00:03:03.479] -> was man die Tiktokisierung der Nachrichten nennt.

[00:03:07.020] -> Und da musst du dich mal fragen.

[00:03:08.979] -> Kannst

[00:03:09.240] -> du eine extrem komplexe Vorlage,

[00:03:11.520] -> sagen wir eine Rentenreform,

[00:03:13.120] -> wirklich in einem 15-Sekunden-Video

[00:03:15.340] -> verstehen?

[00:03:16.080] -> Unmöglich.

[00:03:17.240] -> Du kriegst die Emotion,

[00:03:18.639] -> den schnellen Slogan,

[00:03:19.979] -> aber die Details,

[00:03:20.860] -> die

[00:03:20.960] -> du für die Stimmabgabe brauchst, die fallen weg.

[00:03:23.960] -> Genau.

[00:03:24.520] -> Und man darf ja die wirtschaftliche

[00:03:25.939] -> Seite nicht vergessen.

[00:03:27.539] -> Während du das scrollst,

[00:03:28.800] -> fließen jedes Jahr 2,1 Milliarden Franken

[00:03:31.479] -> an Werbegeldern an diese globalen Plattformen.

[00:03:34.379] -> Geld, das dann den Schweizer Medienhäusern

[00:03:36.560] -> fehlt,

[00:03:36.939] -> um den Journalismus zu finanzieren,

[00:03:38.680] -> der genau diese komplexen Vorlagen verständlich

[00:03:40.860] -> machen soll.

[00:03:41.639] -> Ein Teufelskreis.

[00:03:43.039] -> Okay,

[00:03:43.379] -> all das führt uns jetzt zum Kern der Sache,

[00:03:45.539] -> zum dritten Abschnitt.

[00:03:46.780] -> Was heißt

[00:03:47.159] -> das konkret für dich und für unsere direkte Demokratie?

[00:03:50.960] -> Also, eine erste große Folge ist

[00:03:52.740] -> das Phänomen des Slactivism.

[00:03:55.580] -> Faule Aktivismus.

[00:03:56.900] -> Richtig.

[00:03:57.740] -> Du likest ein politischen Post, teilst

[00:03:59.759] -> eine Story und fühlst dich,

[00:04:01.000] -> als hättest du etwas Wichtiges getan.

[00:04:03.039] -> Es gibt dir das Gefühl

[00:04:03.960] -> von Partizipation,

[00:04:05.360] -> aber ein Like ist eben noch keine Stimme an der Urne.

[00:04:08.199] -> Und das ist

[00:04:08.539] -> ja das Gefährliche.

[00:04:09.560] -> Dieser Slactivism gibt dir das Gefühl,

[00:04:11.900] -> informiert und aktiv zu sein,

[00:04:13.620] -> während dich die Algorithmen gleichzeitig

[00:04:15.680] -> anfälliger für Falschnachrichten machen.

[00:04:17.939] -> Absolut.

[00:04:19.040] -> Nur 55% der Schweizer erkennen Fakes zuverlüssig.

[00:04:22.540] -> Das heißt, du fühlst dich sicherer

[00:04:24.040] -> in deiner Meinung,

[00:04:25.519] -> aber die Basis dieser Meinung wird vielleicht immer wackeliger.

[00:04:28.540] -> Dazu kommt

[00:04:29.139] -> diese emotionale Eskalation.

[00:04:31.500] -> Algorithmen haben gelernt,

[00:04:33.639] -> Wut macht mehr Klicks als Konsens.

[00:04:36.480] -> Das befeuert die affektive Polarisierung.

[00:04:39.500] -> Das heißt,

[00:04:41.319] -> du hältst deine politischen Gegner

[00:04:42.920] -> nicht mehr nur für Leute,

[00:04:44.259] -> die falsch liegen,

[00:04:45.300] -> du fängst an zu glauben,

[00:04:46.579] -> sie seien schlechte

[00:04:47.540] -> Menschen, böse.

[00:04:48.779] -> Und die Macht,

[00:04:49.699] -> Themen zu setzen,

[00:04:50.819] -> das sogenannte Agenda-Setting,

[00:04:53.019] -> verschiebt sich

[00:04:53.639] -> dadurch ja auch komplett.

[00:04:55.500] -> Früher hat eine Redaktion entschieden,

[00:04:57.019] -> was wichtig ist.

[00:04:58.139] -> Und

[00:04:58.439] -> heute?

[00:04:59.160] -> Heute kann ein Thema auf Telegram explodieren und

[00:05:02.899] -> die Politik in Bern muss plötzlich darauf reagieren.

[00:05:05.980] -> Also immer das mal zusammenfassen.

[00:05:07.420] -> Wir haben ja für dich eine ziemliche Reise nachgezeichnet,

[00:05:10.819] -> von dieser einen gemeinsamen Agora,

[00:05:13.319] -> wo alle miteinander geredet haben.

[00:05:15.779] -> Hin zu deinem

[00:05:16.600] -> ganz persönlichen Algorithmus,

[00:05:18.459] -> der dir eine eigene kleine Realität zusammenstellt.

[00:05:22.160] -> Und das

[00:05:22.680] -> hinterlässt dich mit einem ziemlich

[00:05:24.660] -> provokativen Gedanken zum Schluss.

[00:05:26.699] -> Die alten Gatekeeper,

[00:05:27.759] -> die Chefredakteuren sind weg.

[00:05:29.639] -> Der Algorithmus ist der neue Torwächter.

[00:05:32.060] -> Ja. Wenn sein oberstes Ziel aber ist,

[00:05:34.639] -> dich in der App zu halten und nicht dich zu informieren,

[00:05:38.740] -> wer trägt dann die Verantwortung dafür,

[00:05:40.160] -> dass du am Ende ein informierter Bürger bist?

[00:05:42.819] -> Die Plattformen?

[00:05:44.360] -> Der Staat?

[00:05:45.560] -> Oder liegt diese

[00:05:46.300] -> Verantwortung jetzt vielleicht zum ersten Mal in der Geschichte?

[00:05:49.480] -> Ganz allein bei dir.